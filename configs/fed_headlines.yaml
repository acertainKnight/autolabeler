# Fed Headlines Dataset Configuration

name: fed_headlines
labels: ["-99", "-2", "-1", "0", "1", "2"]
text_column: text
input_format: csv

# Pipeline stages
use_relevancy_gate: true
use_candidate_annotation: true
use_typed_rag: true

# Jury: 3 heterogeneous models for diversity
# cost_tier orders models for cascade mode (1=cheapest, called first)
jury_models:
  - provider: google
    model: gemini-2.5-flash
    name: Gemini-Flash
    has_logprobs: false
    self_consistency_samples: 3
    cost_tier: 1
  
  - provider: openai
    model: gpt-4o
    name: GPT-4o
    has_logprobs: true
    self_consistency_samples: 0
    cost_tier: 2
  
  - provider: anthropic
    model: claude-sonnet-4-5-20250929
    name: Claude
    has_logprobs: false
    self_consistency_samples: 3
    cost_tier: 2

# Optional: Relevancy gate (Stage 1)
gate_model:
  provider: openai
  model: gpt-4o-mini
  name: Gate

# Optional: Candidate annotation (Stage 4)
candidate_model:
  provider: anthropic
  model: claude-sonnet-4-5-20250929
  name: Claude-Candidate

# Temperature settings
jury_temperature: 0.1
sc_temperature: 0.3
candidate_temperature: 0.2

# Budget and batching
budget_per_model: 10.0
batch_size: 10

# Cascade mode: call cheapest model first, escalate only when uncertain.
# Saves ~40-60% API cost on easy items. Set use_cascade: true to enable.
use_cascade: false
cascade_confidence_threshold: 0.85  # single-model confidence to accept
cascade_agreement_threshold: 0.80   # two-model agreement to accept

# LIT integration defaults: used by scripts/run_lit.py when flags are omitted.
# All values can be overridden via CLI arguments.
lit:
  embedding_layer: null          # e.g. "encoder.pooler" -- set once you know your model
  enable_gradients: false        # set true to enable token saliency maps
  enable_attention: false        # set true to expose attention head visualisation
  port: 4321

# Post-labeling diagnostics: detect errors without ground truth.
# Set run_post_labeling: true to auto-run after label_dataframe().
# Use scripts/run_diagnostics.py for standalone post-hoc analysis.
diagnostics:
  enabled: true
  run_post_labeling: false  # set true to auto-run after labeling

  # Embedding provider: "local" (sentence-transformers), "openai", or "openrouter"
  embedding_provider: "local"
  embedding_model: "all-MiniLM-L6-v2"
  # embedding_api_key: null  # optional; falls back to env vars (OPENAI_API_KEY / OPENROUTER_API_KEY)
  #
  # Examples for API-based embeddings:
  #   embedding_provider: "openai"
  #   embedding_model: "text-embedding-3-small"   # or "text-embedding-3-large"
  #
  #   embedding_provider: "openrouter"
  #   embedding_model: "openai/text-embedding-3-small"

  nli_model: "cross-encoder/nli-deberta-v3-large"
  outlier_z_threshold: 2.0
  lof_neighbors: 20
  fragmentation_min_cluster_size: 5
  nli_entailment_threshold: 0.5
  duplicate_similarity_threshold: 0.95
  batch_drift_kl_threshold: 0.1
  top_k_suspects: 100
  suspicion_weights:
    embedding_outlier: 0.25
    nli_mismatch: 0.25
    low_confidence: 0.20
    jury_disagreement: 0.15
    rationale_inconsistency: 0.15
