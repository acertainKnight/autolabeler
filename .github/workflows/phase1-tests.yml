name: Phase 1 Test Suite

on:
  push:
    branches: [main, develop, feature/phase1-*]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

jobs:
  # Quick unit tests - run on every push
  unit-tests:
    name: Unit Tests (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run unit tests
        run: |
          pytest tests/test_unit/ \
            -v \
            --tb=short \
            --cov=src/autolabeler \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-fail-under=75 \
            -m unit

      - name: Upload coverage to Codecov
        if: matrix.python-version == '3.11'
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-phase1

  # Integration tests - more thorough
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run integration tests
        run: |
          pytest tests/test_integration/ \
            -v \
            --tb=short \
            --maxfail=3 \
            -m integration

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: |
            .pytest_cache/
            htmlcov/

  # Performance tests - run on specific triggers
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'pull_request' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run performance tests
        run: |
          pytest tests/test_performance/ \
            -v \
            --tb=short \
            -m performance \
            --benchmark-only \
            --benchmark-sort=mean \
            --benchmark-json=benchmark.json

      - name: Store benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark.json

      - name: Comment PR with benchmark results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const benchmark = JSON.parse(fs.readFileSync('benchmark.json', 'utf8'));

            let comment = '## ⚡ Performance Benchmark Results\n\n';
            comment += '| Test | Mean | Min | Max |\n';
            comment += '|------|------|-----|-----|\n';

            for (const test of benchmark.benchmarks) {
              const mean = (test.stats.mean * 1000).toFixed(2);
              const min = (test.stats.min * 1000).toFixed(2);
              const max = (test.stats.max * 1000).toFixed(2);
              comment += `| ${test.name} | ${mean}ms | ${min}ms | ${max}ms |\n`;
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Code quality checks
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run Black
        run: |
          black --check src/ tests/

      - name: Run Ruff
        run: |
          ruff check src/ tests/

      - name: Run codespell
        run: |
          codespell src/ tests/ --skip="*.pyc,*.pkl,*.json"

  # Full test suite - run on main branch
  full-test-suite:
    name: Full Test Suite
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run all tests
        run: |
          pytest tests/ \
            -v \
            --tb=short \
            --cov=src/autolabeler \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --junitxml=junit.xml

      - name: Generate coverage report
        run: |
          coverage report --show-missing

      - name: Upload coverage reports
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: integration
          name: codecov-full

      - name: Upload HTML coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: htmlcov/

      - name: Check coverage threshold
        run: |
          coverage report --fail-under=75

  # Success indicator - all required tests passed
  tests-passed:
    name: All Tests Passed ✅
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, code-quality]
    if: always()

    steps:
      - name: Check test results
        run: |
          if [ "${{ needs.unit-tests.result }}" != "success" ]; then
            echo "Unit tests failed"
            exit 1
          fi
          if [ "${{ needs.integration-tests.result }}" != "success" ]; then
            echo "Integration tests failed"
            exit 1
          fi
          if [ "${{ needs.code-quality.result }}" != "success" ]; then
            echo "Code quality checks failed"
            exit 1
          fi
          echo "All required tests passed! ✅"
